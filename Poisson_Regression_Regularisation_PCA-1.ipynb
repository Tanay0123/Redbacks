{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f05b1c",
   "metadata": {},
   "source": [
    "### Poisson Regression \n",
    "\n",
    "This example provides a complete illustration of Poisson Regression using data from a Kaggle competition. https://www.kaggle.com/code/hongpeiyi/poisson-regression-with-statsmodels/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137d2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler  # For standardizatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b86bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 62.66\n",
      "Mean Absolute Error (MAE): 6.16\n",
      "Root Mean Squared Error (RMSE): 7.92\n",
      "R-squared (R2): -0.00\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "data_path = \"data_poisson.csv\"\n",
    "data = pd.read_csv(data_path, index_col=\"id\")\n",
    "\n",
    "# identify input and output\n",
    "\n",
    "X = data.drop(\"loss\", axis=1)\n",
    "y = data.loss\n",
    "feature_names = X.columns.to_list()\n",
    "\n",
    "# devide data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standarlise\n",
    "standizer = StandardScaler()\n",
    "X_train_S = standizer.fit_transform(X_train)\n",
    "X_test_S = standizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the Poisson Regression model\n",
    "poisson_model = PoissonRegressor()\n",
    "poisson_model.fit(X_train_S, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = poisson_model.predict(X_test_S)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bef50",
   "metadata": {},
   "source": [
    "## Analysis of the baseline-model (you'll need to interpret these values based on your data)\n",
    "Analysis of the first run:\n",
    "MSE, MAE, RMSE give an idea of the prediction error. Lower values are better.\n",
    "R-squared explains the proportion of variance in the output that's predictable from the input. Closer to 1 is better.\n",
    "Consider if the R-squared is reasonable for your data. A low R-squared might indicate a poor fit or that the linear assumptions of the model don't fully hold for count data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd805a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Poisson Regression after PCA (10 components) ---\n",
      "Mean Squared Error (MSE) after PCA: 62.37\n",
      "Mean Absolute Error (MAE) after PCA: 6.14\n",
      "Root Mean Squared Error (RMSE) after PCA: 7.90\n",
      "R-squared (R2) after PCA: 0.00\n"
     ]
    }
   ],
   "source": [
    "# --- Second experiemnt: Poisson Regression after PCA ---\n",
    "print(\"\\n--- Poisson Regression after PCA (10 components) ---\")\n",
    "\n",
    "# 4. Apply PCA to reduce dimensions to 10\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_S)\n",
    "X_test_pca = pca.transform(X_test_S)\n",
    "\n",
    "# Initialize and train Poisson Regression on PCA-transformed data\n",
    "poisson_model_pca = PoissonRegressor()\n",
    "poisson_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the PCA-transformed test set\n",
    "y_pred_pca = poisson_model_pca.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model after PCA\n",
    "mse_pca = mean_squared_error(y_test, y_pred_pca)\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "rmse_pca = np.sqrt(mse_pca)\n",
    "r2_pca = r2_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) after PCA: {mse_pca:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE) after PCA: {mae_pca:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) after PCA: {rmse_pca:.2f}\")\n",
    "print(f\"R-squared (R2) after PCA: {r2_pca:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fec0f",
   "metadata": {},
   "source": [
    "### Note that we used the standarised version of data for before applying PCA, why?\n",
    "\n",
    "PCA is like trying to find the most important \"directions\" or \"axes\" in your data. If some features have much larger values than others, PCA might get tricked into thinking those features are the most important, just because of their large numbers, not because they actually tell you more about the overall structure of the data.\n",
    "\n",
    "Standardization is like converting all your measurements to the same \"standard unit.\" For example, instead of grams and teaspoons, you could think about the ingredient's value relative to its average and how spread out it is. This way, PCA focuses on the real variance and relationships in the data, not just the differences in the scales of the original measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f630e0f",
   "metadata": {},
   "source": [
    "## Comparison of Poisson Regression Performance: Baseline vs. PCA (10 Components)\n",
    "\n",
    "Here's a comparison of the Poisson Regression model's performance before applying PCA (Baseline) and after applying PCA with 10 components:\n",
    "\n",
    "| Metric                 | Baseline (Original Features) | PCA (10 Components) |\n",
    "| ---------------------- | ---------------------------- | --------------------- |\n",
    "| Mean Squared Error (MSE) | 62.66                        | 62.08                 |\n",
    "| Mean Absolute Error (MAE) | 6.16                         | 6.12                  |\n",
    "| Root Mean Squared Error (RMSE) | 7.92                         | 7.88                  |\n",
    "| R-squared (R2)         | -0.00                        | 0.00                  |\n",
    "\n",
    "**Analysis of the Metrics:**\n",
    "\n",
    "* **Mean Squared Error (MSE):** This measures the average of the squared differences between the predicted and actual values. A lower MSE indicates better performance.\n",
    "    * The MSE decreased slightly from 62.66 in the baseline to 62.08 after PCA. This suggests a marginal improvement in the overall squared error.\n",
    "\n",
    "* **Mean Absolute Error (MAE):** This measures the average of the absolute differences between the predicted and actual values. It's easier to interpret than MSE as it's in the same units as the target variable.\n",
    "    * The MAE also decreased slightly from 6.16 to 6.12 after PCA, indicating a small reduction in the average absolute error.\n",
    "\n",
    "* **Root Mean Squared Error (RMSE):** This is the square root of the MSE. It provides a measure of the typical magnitude of the errors and is also in the same units as the target variable.\n",
    "    * The RMSE decreased slightly from 7.92 to 7.88 after PCA, suggesting a minor reduction in the typical error magnitude.\n",
    "\n",
    "* **R-squared (R2):** This metric represents the proportion of the variance in the dependent variable (the target, 'loss' in this case) that is predictable from the independent variables (the features).\n",
    "    * **Baseline R-squared: -0.00**\n",
    "        * An R-squared of 0 indicates that the model does not explain any of the variability of the response data around its mean. In other words, the model is no better than simply predicting the average value of the target variable for all predictions.\n",
    "        * A negative R-squared, while rare, indicates that the model fits the data worse than a horizontal line at the mean of the dependent variable. This suggests that the model is making very poor predictions.\n",
    "    * **PCA (10 Components) R-squared: 0.00**\n",
    "        * The R-squared increased slightly to 0.00 after applying PCA. While this is an improvement over a negative value, it still signifies that the model explains none of the variance in the target variable. The model is essentially performing no better than a simple average prediction.\n",
    "\n",
    "**Overall Judgment:**\n",
    "\n",
    "Applying PCA with 10 components has resulted in a very slight improvement in the error metrics (MSE, MAE, RMSE). However, the R-squared value remains at or very close to zero.\n",
    "\n",
    "**What does this mean, especially the R-squared?**\n",
    "\n",
    "The R-squared values are the most concerning aspect of these results. An R-squared of 0 (or a negative value) indicates that the Poisson Regression model, both with the original features and after PCA with 10 components, is **not effectively capturing the relationship between the input features and the target variable ('loss').**\n",
    "\n",
    "* **The model is not explaining any of the reasons why the 'loss' values vary.** It's essentially making predictions that are no better than just guessing the average 'loss' value.\n",
    "* **The features, even after reducing dimensionality with PCA, are not strongly linearly related to the (log of the expected) 'loss' in a way that the Poisson Regression model can learn effectively.**\n",
    "\n",
    "**In conclusion:** While PCA with 10 components has slightly reduced the prediction errors, it hasn't addressed the fundamental issue of the model's inability to explain the variance in the target variable. The model is still performing very poorly, as indicated by the near-zero R-squared. Your students should continue to explore other potential issues like the appropriateness of the Poisson Regression model for this data, the quality of the features, or the need for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c846fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Poisson Regression with L2 Regularization ---\n",
      "Mean Squared Error (MSE) with L2: 62.66\n",
      "Mean Absolute Error (MAE) with L2: 6.16\n",
      "Root Mean Squared Error (RMSE) with L2: 7.92\n",
      "R-squared (R2) with L2: -0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Poisson Regression with Regularization ---\n",
    "print(\"\\n--- Poisson Regression with L2 Regularization ---\")\n",
    "\n",
    "# Initialize and train the Poisson Regression model with L2 regularization\n",
    "# alpha controls the strength of the regularization (higher alpha means stronger)\n",
    "poisson_model_reg_l2 = PoissonRegressor(alpha=1.0)\n",
    "poisson_model_reg_l2.fit(X_train_S, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_reg_l2 = poisson_model_reg_l2.predict(X_test_S)\n",
    "\n",
    "# Evaluate the model with L2 regularization\n",
    "mse_reg_l2 = mean_squared_error(y_test, y_pred_reg_l2)\n",
    "mae_reg_l2 = mean_absolute_error(y_test, y_pred_reg_l2)\n",
    "rmse_reg_l2 = np.sqrt(mse_reg_l2)\n",
    "r2_reg_l2 = r2_score(y_test, y_pred_reg_l2)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) with L2: {mse_reg_l2:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE) with L2: {mae_reg_l2:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) with L2: {rmse_reg_l2:.2f}\")\n",
    "print(f\"R-squared (R2) with L2: {r2_reg_l2:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593813c5",
   "metadata": {},
   "source": [
    "## Why Regularization Makes Sense for Linear Models (Despite Their Simplicity)\n",
    "\n",
    "It's understandable why you might think regularization is only for more complex models. Linear models like Linear Regression and Poisson Regression seem straightforward, so why would we need to \"regularize\" them?\n",
    "\n",
    "Here's the breakdown:\n",
    "\n",
    "**1. Preventing Overfitting, Even in Linear Models:**\n",
    "\n",
    "* **What is Overfitting?** Overfitting happens when a model learns the training data *too well*, including the noise and random fluctuations. This leads to excellent performance on the training data but poor performance on new, unseen data (like the test set).\n",
    "* **Can Linear Models Overfit?** Yes, especially when:\n",
    "    * **You have a large number of features compared to the number of data points.** In such cases, the linear model can become very flexible and essentially memorize the training data.\n",
    "    * **There's multicollinearity (high correlation between features).** This can cause the model to assign very large and unstable coefficients to the correlated features.\n",
    "* **How Regularization Helps:** Regularization adds a penalty to the model's complexity, specifically by penalizing large coefficient values. This forces the model to find a simpler solution that generalizes better to new data.\n",
    "\n",
    "**2. Improving Generalization:**\n",
    "\n",
    "* The primary goal of any machine learning model is to generalize well to unseen data. Regularization is a powerful tool to achieve this.\n",
    "* By shrinking or setting coefficients to zero, regularization makes the model less sensitive to the specific patterns in the training data, including noise. This often leads to better performance on the test set.\n",
    "\n",
    "**3. Addressing Multicollinearity:**\n",
    "\n",
    "* As mentioned earlier, multicollinearity can lead to unstable and hard-to-interpret coefficients in linear models.\n",
    "* **L2 Regularization (Ridge Regression):** Shrinks the coefficients of correlated features towards each other, reducing their individual impact and stabilizing the model.\n",
    "* **L1 Regularization (Lasso Regression):** Can drive the coefficients of less important correlated features to exactly zero, effectively performing feature selection and simplifying the model.\n",
    "\n",
    "**4. Feature Selection (Especially with L1):**\n",
    "\n",
    "* Even if you don't have a huge number of features, L1 regularization can be useful for identifying the most important features by setting the coefficients of less relevant features to zero. This can lead to a more interpretable and potentially more efficient model.\n",
    "\n",
    "**Think of it like this:**\n",
    "\n",
    "Imagine you're trying to fit a straight line through some data points.\n",
    "\n",
    "* **Without Regularization:** If you have a lot of data points and maybe some outliers, the line might try to get very close to every single point, even the outliers. This can result in a wiggly line that doesn't represent the overall trend well.\n",
    "* **With Regularization:** Regularization forces the line to be smoother and less influenced by individual noisy points. It finds a more general trend that is likely to work better on new data.\n",
    "\n",
    "**In the context of Poisson Regression:**\n",
    "\n",
    "Poisson Regression models the relationship between features and the log of the expected count. Even with this specific form, it's still a linear model in terms of the coefficients. Regularization can help prevent overfitting if you have many features or multicollinearity in your count data modeling.\n",
    "\n",
    "**Key Takeaway:**\n",
    "\n",
    "Regularization isn't just for complex models like neural networks or decision trees. It's a valuable technique for linear models to improve their generalization ability, handle multicollinearity, and potentially perform feature selection. It's about finding the right balance between fitting the training data well and creating a model that performs reliably on new data.\n",
    "\n",
    "So, while linear models are simpler than some others, they can still benefit significantly from the application of regularization techniques like L1 and L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3a2f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Poisson Regression with L2 Regularization ---\n",
      "Mean Squared Error (MSE) with L2: 62.11\n",
      "Mean Absolute Error (MAE) with L2: 6.13\n",
      "Root Mean Squared Error (RMSE) with L2: 7.88\n",
      "R-squared (R2) with L2: 0.00\n"
     ]
    }
   ],
   "source": [
    "# --- Poisson Regression with Regularization and parameter tuning ---\n",
    "print(\"\\n--- Poisson Regression with L2 Regularization ---\")\n",
    "\n",
    "# Initialize and train the Poisson Regression model with L2 regularization\n",
    "# alpha controls the strength of the regularization (higher alpha means stronger)\n",
    "poisson_model_reg_l2 = PoissonRegressor(alpha=10)\n",
    "poisson_model_reg_l2.fit(X_train_S, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_reg_l2 = poisson_model_reg_l2.predict(X_test_S)\n",
    "\n",
    "# Evaluate the model with L2 regularization\n",
    "mse_reg_l2 = mean_squared_error(y_test, y_pred_reg_l2)\n",
    "mae_reg_l2 = mean_absolute_error(y_test, y_pred_reg_l2)\n",
    "rmse_reg_l2 = np.sqrt(mse_reg_l2)\n",
    "r2_reg_l2 = r2_score(y_test, y_pred_reg_l2)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) with L2: {mse_reg_l2:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE) with L2: {mae_reg_l2:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) with L2: {rmse_reg_l2:.2f}\")\n",
    "print(f\"R-squared (R2) with L2: {r2_reg_l2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eda0b0",
   "metadata": {},
   "source": [
    "### As the result indicates the regularisation had a positive impact on the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de1ee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) after PCA with L2: 62.63\n",
      "Mean Absolute Error (MAE) after PCA with L2: 6.14\n",
      "Root Mean Squared Error (RMSE) after PCA with L2: 7.91\n",
      "R-squared (R2) after PCA with L2: -0.00\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA to reduce dimensions to 10, then include regularisation\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_S)\n",
    "X_test_pca = pca.transform(X_test_S)\n",
    "\n",
    "# Initialize and train Poisson Regression on PCA-transformed data with L2 regularization\n",
    "poisson_model_pca_reg = PoissonRegressor(alpha=0.001)  # Adding L2 regularization with alpha=1.0\n",
    "poisson_model_pca_reg.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the PCA-transformed test set\n",
    "y_pred_pca_reg = poisson_model_pca_reg.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model after PCA with L2 regularization\n",
    "mse_pca_reg = mean_squared_error(y_test, y_pred_pca_reg)\n",
    "mae_pca_reg = mean_absolute_error(y_test, y_pred_pca_reg)\n",
    "rmse_pca_reg = np.sqrt(mse_pca_reg)\n",
    "r2_pca_reg = r2_score(y_test, y_pred_pca_reg)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) after PCA with L2: {mse_pca_reg:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE) after PCA with L2: {mae_pca_reg:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) after PCA with L2: {rmse_pca_reg:.2f}\")\n",
    "print(f\"R-squared (R2) after PCA with L2: {r2_pca_reg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3ff18",
   "metadata": {},
   "source": [
    "## Why Regularization Hurts After PCA: Summary for Students\n",
    "\n",
    "It's interesting that regularization helps your model with the original data but makes it worse after applying PCA! Here's why this might be happening:\n",
    "\n",
    "**Think of it like this:**\n",
    "\n",
    "* **Original Data:** Imagine you have a messy room with lots of stuff, some useful, some junk. Regularization is like tidying up, getting rid of the junk (less important features) and organizing the useful stuff.\n",
    "* **PCA:** PCA is like taking a picture of the room that captures the most important things in a simplified way. You lose some details, but you see the main structure.\n",
    "\n",
    "**Why Regularization Might Hurt *After* PCA:**\n",
    "\n",
    "1.  **PCA Already Simplified:** PCA already reduced the number of \"things\" (features) and kept what it thought was most important. Trying to \"tidy up\" *again* after PCA might be like throwing away something important that PCA decided to keep in the picture.\n",
    "2.  **PCA Changes the \"Things\":** The \"things\" in the picture (principal components) are combinations of the original items. Regularization might not understand these combinations and could accidentally remove important parts.\n",
    "3.  **Too Much Cleaning:** If you strongly regularize after PCA, you might be cleaning *too much*, making the model too simple and missing important patterns.\n",
    "\n",
    "**In Simple Words:**\n",
    "\n",
    "* Regularization on the original data helps manage messy features.\n",
    "* PCA already tries to make the features cleaner and fewer.\n",
    "* Applying regularization *after* PCA can be like overdoing it and removing important information that PCA kept.\n",
    "\n",
    "**What to Do:**\n",
    "\n",
    "* **Experiment:** Try using less regularization (smaller `alpha`) after PCA.\n",
    "* **Think Carefully:** PCA and regularization both try to simplify the model. You might not need to do both strongly.\n",
    "* **Look at Results:** Always check your scores (like R-squared) to see what works best.\n",
    "\n",
    "**Key Idea:** PCA and regularization can sometimes overlap in what they do. If PCA has already simplified the data, adding too much regularization might be unnecessary or even harmful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfeb6634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Polynomial Poisson Regression (Degree 2) - Increased max_iter ---\n",
      "Mean Squared Error (MSE): 86.67\n",
      "Mean Absolute Error (MAE): 6.94\n",
      "Root Mean Squared Error (RMSE): 9.31\n",
      "R-squared (R2): -0.39\n",
      "\n",
      "--- Polynomial Poisson Regression (Degree 2) with L2 Regularization - Increased max_iter ---\n",
      "Mean Squared Error (MSE): 86.67\n",
      "Mean Absolute Error (MAE): 6.94\n",
      "Root Mean Squared Error (RMSE): 9.31\n",
      "R-squared (R2): -0.39\n",
      "\n",
      "--- Polynomial Poisson Regression (Degree 2) with PCA (n_components=10) and L2 Regularization ---\n",
      "Mean Squared Error (MSE): 62.36\n",
      "Mean Absolute Error (MAE): 6.15\n",
      "Root Mean Squared Error (RMSE): 7.90\n",
      "R-squared (R2): 0.00\n"
     ]
    }
   ],
   "source": [
    "# Polynomial poisson Regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = \"data_poisson.csv\"\n",
    "data = pd.read_csv(data_path, index_col=\"id\")\n",
    "\n",
    "# Identify input and output\n",
    "X = data.drop(\"loss\", axis=1)\n",
    "y = data.loss\n",
    "feature_names = X.columns.to_list()\n",
    "\n",
    "# Divide data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 1. Polynomial Poisson Regression (Degree 2) - Increased max_iter ---\n",
    "print(\"\\n--- Polynomial Poisson Regression (Degree 2) - Increased max_iter ---\")\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Scale the polynomial features\n",
    "scaler_poly = StandardScaler()\n",
    "X_train_poly_scaled = scaler_poly.fit_transform(X_train_poly)\n",
    "X_test_poly_scaled = scaler_poly.transform(X_test_poly)\n",
    "\n",
    "# Initialize and train Poisson Regression with increased max_iter\n",
    "poisson_model_poly = PoissonRegressor(max_iter=500)\n",
    "poisson_model_poly.fit(X_train_poly_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly = poisson_model_poly.predict(X_test_poly_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "mae_poly = mean_absolute_error(y_test, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mse_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse_poly:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_poly:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_poly:.2f}\")\n",
    "print(f\"R-squared (R2): {r2_poly:.2f}\")\n",
    "\n",
    "# --- 2. Polynomial Poisson Regression (Degree 2) with L2 Regularization - Increased max_iter ---\n",
    "print(\"\\n--- Polynomial Poisson Regression (Degree 2) with L2 Regularization - Increased max_iter ---\")\n",
    "\n",
    "# Initialize and train Poisson Regression with L2 regularization and increased max_iter\n",
    "poisson_model_poly_reg = PoissonRegressor(alpha=1.0, max_iter=500)\n",
    "poisson_model_poly_reg.fit(X_train_poly_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly_reg = poisson_model_poly_reg.predict(X_test_poly_scaled)  # Corrected variable name\n",
    "\n",
    "# Evaluate\n",
    "mse_poly_reg = mean_squared_error(y_test, y_pred_poly_reg)\n",
    "mae_poly_reg = mean_absolute_error(y_test, y_pred_poly_reg)\n",
    "rmse_poly_reg = np.sqrt(mse_poly_reg)\n",
    "r2_poly_reg = r2_score(y_test, y_pred_poly_reg)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse_poly_reg:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_poly_reg:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_poly_reg:.2f}\")\n",
    "print(f\"R-squared (R2): {r2_poly_reg:.2f}\")\n",
    "\n",
    "# --- 3. Polynomial Poisson Regression (Degree 2) with PCA (n_components=10) and L2 Regularization ---\n",
    "print(\"\\n--- Polynomial Poisson Regression (Degree 2) with PCA (n_components=10) and L2 Regularization ---\")\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_poly_scaled)\n",
    "X_test_pca = pca.transform(X_test_poly_scaled)\n",
    "\n",
    "# Initialize and train Poisson Regression with L2 regularization\n",
    "poisson_model_pca_reg = PoissonRegressor(alpha=1.0)\n",
    "poisson_model_pca_reg.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_pca_reg = poisson_model_pca_reg.predict(X_test_pca)\n",
    "\n",
    "# Evaluate\n",
    "mse_pca_reg = mean_squared_error(y_test, y_pred_pca_reg)\n",
    "mae_pca_reg = mean_absolute_error(y_test, y_pred_pca_reg)\n",
    "rmse_pca_reg = np.sqrt(mse_pca_reg)\n",
    "r2_pca_reg = r2_score(y_test, y_pred_pca_reg)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse_pca_reg:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_pca_reg:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_pca_reg:.2f}\")\n",
    "print(f\"R-squared (R2): {r2_pca_reg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2d2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
