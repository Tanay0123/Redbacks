{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSZDj6VNDDQ0",
    "outputId": "b668f523-5e06-4b86-ede2-148644ef9773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Successfully loaded data from Drive. Shape: (93744, 11)\n",
      "        country  brand_name month  months_postgx       volume  n_gxs  \\\n",
      "0  COUNTRY_B6AE  BRAND_1C1E   Jul            -24  272594.3921      0   \n",
      "1  COUNTRY_B6AE  BRAND_1C1E   Aug            -23  351859.3103      0   \n",
      "2  COUNTRY_B6AE  BRAND_1C1E   Sep            -22  447953.4813      0   \n",
      "3  COUNTRY_B6AE  BRAND_1C1E   Oct            -21  411543.2924      0   \n",
      "4  COUNTRY_B6AE  BRAND_1C1E   Nov            -20  774594.4542      0   \n",
      "\n",
      "                                  ther_area  hospital_rate main_package  \\\n",
      "0  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
      "1  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
      "2  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
      "3  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
      "4  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
      "\n",
      "   biological  small_molecule  \n",
      "0       False            True  \n",
      "1       False            True  \n",
      "2       False            True  \n",
      "3       False            True  \n",
      "4       False            True  \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# file_path = '/content/drive/MyDrive/merged_cleaned_train_data.csv'\n",
    "\n",
    "# try:\n",
    "#     df = pd.read_csv(file_path)\n",
    "\n",
    "#     if 'Unnamed: 0' in df.columns:\n",
    "#         df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "#     print(f\"Successfully loaded data from Drive. Shape: {df.shape}\")\n",
    "#     print(df.head())\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: File not found at {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YYyAB32W9wm"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.1.2-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.4/72.0 MB 12.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 4.7/72.0 MB 11.9 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 5.2/72.0 MB 11.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 7.3/72.0 MB 8.7 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 10.0/72.0 MB 9.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 12.3/72.0 MB 9.8 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 14.7/72.0 MB 10.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 17.3/72.0 MB 10.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 19.9/72.0 MB 10.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 22.3/72.0 MB 10.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 24.6/72.0 MB 10.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 27.0/72.0 MB 10.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 29.6/72.0 MB 10.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 32.2/72.0 MB 10.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 34.6/72.0 MB 10.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 37.0/72.0 MB 11.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 39.3/72.0 MB 11.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 41.9/72.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 44.3/72.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 46.7/72.0 MB 11.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 49.3/72.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 51.9/72.0 MB 11.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 54.3/72.0 MB 11.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 56.9/72.0 MB 11.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 59.5/72.0 MB 11.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 61.9/72.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 64.5/72.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 66.8/72.0 MB 11.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 69.5/72.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 11.0 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yZjHfXToW_lV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "# from google.colab import drive\n",
    "# from google.colab import files\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD3H-PDrXFPn"
   },
   "source": [
    "Best parameters (Taken by randomsearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A6qh9PiiXCP4"
   },
   "outputs": [],
   "source": [
    "FINAL_XGB_PARAMS = { # needs higher n_estimators to find subtle patterns and avoid underfitting\n",
    "    'n_estimators': 4000, # need to be higher like 2000\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 5,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'reg_lambda' : 2.0,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "\n",
    "# separated parameter for s2\n",
    "FINAL_XGB_PARAMS_s2 = { #converges quickly due to the highly predictive features so needs lower n_estimators to prevent overfitting\n",
    "    'n_estimators': 600, #  400-600 would be good\n",
    "    'learning_rate': 0.04, # thinking of changing this to a lower value\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 5,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'reg_lambda' : 2.0,\n",
    "    'objective': 'reg:squarederror'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCTQMtJoSKft"
   },
   "source": [
    "Path files and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p0V21xHDSJfH"
   },
   "outputs": [],
   "source": [
    "# DRIVE_TRAIN_PATH = '/content/drive/MyDrive/merged_cleaned_train_data.csv'\n",
    "# TEST_FILE_NAME = '/content/drive/MyDrive/novartis_test_data_merged.csv'\n",
    "# KEYS_STATIC = ['country', 'brand_name']\n",
    "# static_cols = ['ther_area', 'main_package', 'biological', 'small_molecule', 'hospital_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE_TRAIN_PATH = 'Data/TRAIN/merged_cleaned_train_data.csv'\n",
    "TEST_FILE_NAME = 'Data/TEST/novartis_test_data_merged.csv'\n",
    "KEYS_STATIC = ['country', 'brand_name']\n",
    "static_cols = ['ther_area', 'main_package', 'biological', 'small_molecule', 'hospital_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J14yaUbOO1Se",
    "outputId": "1f11ac81-c04c-4b86-e132-b154d80bbee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training on full dataset\n",
      "Final Tuned Models Trained Successfully.\n"
     ]
    }
   ],
   "source": [
    "# helper functions\n",
    "def log_transform(df): return np.log1p(df) # shrinks high volume data and stretches the small ones to make the data easier to learn from\n",
    "def inverse_transform(arr): return np.expm1(arr) # bias correction when predictions were too low\n",
    "\n",
    "# load data and prep\n",
    "# drive.mount('/content/drive')\n",
    "df = pd.read_csv(DRIVE_TRAIN_PATH)\n",
    "\n",
    "#drops any redundant columns\n",
    "if 'Unnamed: 0' in df.columns: df = df.drop(columns=['Unnamed: 0'])\n",
    "# replaces missing values with mean value to preserve row while maintaining the overall distribution of the variable\n",
    "df['hospital_rate'] = df['hospital_rate'].fillna(df['hospital_rate'].mean())\n",
    "\n",
    "# feature engineering\n",
    "df_pre = df[(df['months_postgx'] >= -12) & (df['months_postgx'] <= -1)] # uses 12 months immediately preceding generic entry\n",
    "avg_j = df_pre.groupby(KEYS_STATIC)['volume'].mean().reset_index(name='Avg_j') # used because PE is defined as the mean of the last 12 months before generic entry\n",
    "df_static = df.drop_duplicates(subset=KEYS_STATIC)[KEYS_STATIC + static_cols]\n",
    "X_base = pd.merge(df_static, avg_j, on=KEYS_STATIC, how='inner').set_index(KEYS_STATIC) #for scenario 1 prediction (uninformed features)\n",
    "\n",
    "# X1, Y1 (M0..M23)\n",
    "df_Y1 = df[(df['months_postgx'] >= 0) & (df['months_postgx'] <= 23)] #filters raw data to only include 24 months after generic entry\n",
    "Y1 = df_Y1.pivot_table(index=KEYS_STATIC, columns='months_postgx', values='volume').fillna(np.nan) # target matrix for y1\n",
    "X1 = pd.merge(X_base, Y1, left_index=True, right_index=True, how='inner').iloc[:, :-24]\n",
    "Y1 = Y1.loc[X1.index] # created final target natrix\n",
    "\n",
    "# X2, Y2 (M6..M23 with M0-M5 features) - Scenario 2 (Informed)\n",
    "\n",
    "# feature engineering for scenario 2\n",
    "df_M0_M5 = df[(df['months_postgx'] >= 0) & (df['months_postgx'] <= 5)].pivot_table(index=KEYS_STATIC, columns='months_postgx', values='volume').fillna(np.nan) # filters data to isolate actual volumes\n",
    "M0_M5_feats = pd.concat([df_M0_M5[0].rename('Vol_M0'), df_M0_M5[5].rename('Vol_M5'), (df_M0_M5[0] - df_M0_M5[5]).rename('Erosion_M0_M5')], axis=1) # calculates initial erosion features months 0-6 post entry volume\n",
    "X2 = pd.merge(X_base, M0_M5_feats, left_index=True, right_index=True, how='inner') # includes all input information necessary for the informed forecast from month 6 onwards\n",
    "\n",
    "df_Y2 = df[(df['months_postgx'] >= 6) & (df['months_postgx'] <= 23)] # 18 month prediction\n",
    "Y2 = df_Y2.pivot_table(index=KEYS_STATIC, columns='months_postgx', values='volume').fillna(np.nan)\n",
    "Y2 = Y2.loc[X2.index] # created final target matrix\n",
    "\n",
    "# encoding to prep data for XGBoost\n",
    "X1['country_feat'] = X1.index.get_level_values('country')\n",
    "X2['country_feat'] = X2.index.get_level_values('country')\n",
    "cat_cols = X1.select_dtypes(include=['object']).columns.tolist()\n",
    "preprocessor = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)], remainder='passthrough') # applied one hot encoding to use XGBoost efficiently\n",
    "# fit and transform( maximizing data utilization)\n",
    "X1_enc = preprocessor.fit_transform(X1).astype(float)\n",
    "X2_enc = preprocessor.transform(X2).astype(float)\n",
    "\n",
    "# Full Data Buckets (for training weights)\n",
    "Y1_norm = Y1.div(X_base['Avg_j'], axis=0) # calculate normalized volume\n",
    "buckets = np.where(Y1_norm.mean(axis=1) <= 0.25, 1, 2) # if bucket labelled 1 = high erosion\n",
    "weights_full = np.where(buckets == 1, 2.0, 1.0) # this forces model to treat high erosion drugs error twice as costly\n",
    "\n",
    "# Model Training\n",
    "print(\"\\n training on full dataset\")\n",
    "Y1_log = log_transform(Y1)\n",
    "Y2_log = log_transform(Y2)\n",
    "\n",
    "# Model 1 (S1)\n",
    "m1 = MultiOutputRegressor(xgb.XGBRegressor(**FINAL_XGB_PARAMS))\n",
    "m1.fit(X1_enc, Y1_log.values, sample_weight=weights_full)\n",
    "\n",
    "# Model 2 (S2)\n",
    "m2 = MultiOutputRegressor(xgb.XGBRegressor(**FINAL_XGB_PARAMS_s2))\n",
    "m2.fit(X2_enc, Y2_log.values, sample_weight=weights_full)\n",
    "print(\"Final Tuned Models Trained Successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILE_NAME = \"Data/TEST/novartis_test_data_merged_full(New).csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "u02tj4ZUf6ce",
    "outputId": "5e886636-bf66-4547-98e0-59143f950871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Complete! Final CSV '4000_0.01_l2reg.csv' created with 7488 rows.\n"
     ]
    }
   ],
   "source": [
    "# Submission CSV\n",
    "try:\n",
    "    df_test = pd.read_csv(TEST_FILE_NAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{TEST_FILE_NAME}' not found.\")\n",
    "    raise\n",
    "\n",
    "# initial cleaning (Test)\n",
    "df_test['n_gxs'] = df_test['n_gxs'].fillna(0.0)\n",
    "df_test['hospital_rate'] = df_test['hospital_rate'].fillna(df_test['hospital_rate'].mean())\n",
    "\n",
    "# test Feature Engineering (perfectly match train FE)\n",
    "df_pre_t = df_test[(df_test['months_postgx'] >= -12) & (df_test['months_postgx'] <= -1)]\n",
    "avg_j_t = df_pre_t.groupby(KEYS_STATIC)['volume'].mean().reset_index(name='Avg_j')\n",
    "df_static_t = df_test.drop_duplicates(subset=KEYS_STATIC)[KEYS_STATIC + static_cols]\n",
    "X_base_t = pd.merge(df_static_t, avg_j_t, on=KEYS_STATIC, how='inner').set_index(KEYS_STATIC)\n",
    "df_M0_M5_t = df_test[(df_test['months_postgx'] >= 0) & (df_test['months_postgx'] <= 5)].pivot_table(index=KEYS_STATIC, columns='months_postgx', values='volume').fillna(np.nan)\n",
    "M0_M5_feats_t = pd.concat([df_M0_M5_t[0].rename('Vol_M0'), df_M0_M5_t[5].rename('Vol_M5'), (df_M0_M5_t[0] - df_M0_M5_t[5]).rename('Erosion_M0_M5')], axis=1)\n",
    "\n",
    "# split Test drugs\n",
    "s2_drugs = M0_M5_feats_t.dropna().index\n",
    "s1_drugs = X_base_t.index.difference(s2_drugs)\n",
    "\n",
    "# prepare final prediction matrices\n",
    "X_test_S1 = X_base_t.loc[s1_drugs].copy()\n",
    "X_test_S1['country_feat'] = X_test_S1.index.get_level_values('country')\n",
    "X_test_S2 = pd.merge(X_base_t, M0_M5_feats_t, left_index=True, right_index=True, how='inner').loc[s2_drugs]\n",
    "X_test_S2['country_feat'] = X_test_S2.index.get_level_values('country')\n",
    "\n",
    "# encoding\n",
    "X_test_S1_enc = preprocessor.transform(X_test_S1).astype(float)\n",
    "X_test_S2_enc = preprocessor.transform(X_test_S2).astype(float)\n",
    "\n",
    "# predict\n",
    "p_s1_log = m1.predict(X_test_S1_enc)\n",
    "p_s2_log = m2.predict(X_test_S2_enc)\n",
    "\n",
    "p_s1_final = np.maximum(0, inverse_transform(p_s1_log))\n",
    "p_s2_final = np.maximum(0, inverse_transform(p_s2_log))\n",
    "\n",
    "# Combine and Save\n",
    "df_p1 = pd.DataFrame(p_s1_final, index=X_test_S1.index, columns=[f'Volume_M{i}' for i in range(24)])\n",
    "df_p2 = pd.DataFrame(p_s2_final, index=X_test_S2.index, columns=[f'Volume_M{i}' for i in range(6, 24)])\n",
    "\n",
    "p1_long = df_p1.stack().reset_index()\n",
    "p1_long.columns = ['country', 'brand_name', 'm_col', 'volume']\n",
    "p1_long['months_postgx'] = p1_long['m_col'].str.replace('Volume_M', '').astype(int)\n",
    "\n",
    "p2_long = df_p2.stack().reset_index()\n",
    "p2_long.columns = ['country', 'brand_name', 'm_col', 'volume']\n",
    "p2_long['months_postgx'] = p2_long['m_col'].str.replace('Volume_M', '').astype(int)\n",
    "\n",
    "final_submission = pd.concat([p1_long[['country', 'brand_name', 'months_postgx', 'volume']], p2_long[['country', 'brand_name', 'months_postgx', 'volume']]])\n",
    "\n",
    "filename = '4000_0.01_l2reg.csv'\n",
    "final_submission.to_csv(filename, index=False)\n",
    "# files.download(filename)\n",
    "print(f\"Submission Complete! Final CSV '{filename}' created with {len(final_submission)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1et6rQUDRgad"
   },
   "source": [
    "Metrics calculation (From Novartis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EfELVPo3XQi4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# successfully defined:\n",
    "# X1_val, Y1_val, X2_val, Y2_val (Validation feature/target sets)\n",
    "# m1, m2 (Trained XGBoost models)\n",
    "# df_aux_val (Auxiliary data with bucket/avg_vol)\n",
    "# log_transform, inverse_transform (Helper functions)\n",
    "\n",
    "def _compute_pe_phase1a(group: pd.DataFrame) -> float:\n",
    "\n",
    "    avg_vol = group[\"avg_vol\"].iloc[0];\n",
    "    if avg_vol == 0 or np.isnan(avg_vol): return np.nan\n",
    "    def sum_abs_diff(month_start: int, month_end: int) -> float:\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)];\n",
    "        return (subset[\"volume_actual\"] - subset[\"volume_predict\"]).abs().sum()\n",
    "    def abs_sum_diff(month_start: int, month_end: int) -> float:\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)];\n",
    "        return abs(subset[\"volume_actual\"].sum() - subset[\"volume_predict\"].sum())\n",
    "    term1 = 0.2 * sum_abs_diff(0, 23) / (24 * avg_vol);\n",
    "    term2 = 0.5 * abs_sum_diff(0, 5) / (6 * avg_vol);\n",
    "    term3 = 0.2 * abs_sum_diff(6, 11) / (6 * avg_vol);\n",
    "    term4 = 0.1 * abs_sum_diff(12, 23) / (12 * avg_vol);\n",
    "    return term1 + term2 + term3 + term4\n",
    "\n",
    "def compute_metric1(df_actual: pd.DataFrame, df_pred: pd.DataFrame, df_aux: pd.DataFrame) -> float:\n",
    "\n",
    "    merged = df_actual.merge(df_pred, on=[\"country\", \"brand_name\", \"months_postgx\"], how=\"inner\", suffixes=(\"_actual\", \"_predict\")).merge(df_aux, on=[\"country\", \"brand_name\"], how=\"left\");\n",
    "    merged[\"start_month\"] = merged.groupby([\"country\", \"brand_name\"])[\"months_postgx\"].transform(\"min\");\n",
    "    merged = merged[merged[\"start_month\"] == 0].copy();\n",
    "    pe_results = (merged.groupby([\"country\", \"brand_name\", \"bucket\"]).apply(_compute_pe_phase1a).reset_index(name=\"PE\"));\n",
    "    bucket1 = pe_results[pe_results[\"bucket\"] == 1]; bucket2 = pe_results[pe_results[\"bucket\"] == 2];\n",
    "    n1 = len(bucket1); n2 = len(bucket2); score = 0;\n",
    "    if n1 > 0: score += (2 / n1) * bucket1[\"PE\"].sum();\n",
    "    if n2 > 0: score += (1 / n2) * bucket2[\"PE\"].sum();\n",
    "    return round(score, 4)\n",
    "\n",
    "def _compute_pe_phase1b(group: pd.DataFrame) -> float:\n",
    "\n",
    "    avg_vol = group[\"avg_vol\"].iloc[0];\n",
    "    if avg_vol == 0 or np.isnan(avg_vol): return np.nan\n",
    "    def sum_abs_diff(month_start: int, month_end: int) -> float:\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)];\n",
    "        return (subset[\"volume_actual\"] - subset[\"volume_predict\"]).abs().sum()\n",
    "    def abs_sum_diff(month_start: int, month_end: int) -> float:\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)];\n",
    "        return abs(subset[\"volume_actual\"].sum() - subset[\"volume_predict\"].sum())\n",
    "    term1 = 0.2 * sum_abs_diff(6, 23) / (18 * avg_vol);\n",
    "    term2 = 0.5 * abs_sum_diff(6, 11) / (6 * avg_vol);\n",
    "    term3 = 0.3 * abs_sum_diff(12, 23) / (12 * avg_vol);\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def compute_metric2(df_actual: pd.DataFrame, df_pred: pd.DataFrame, df_aux: pd.DataFrame) -> float:\n",
    "\n",
    "    merged_data = df_actual.merge(df_pred, on=[\"country\", \"brand_name\", \"months_postgx\"], how=\"inner\", suffixes=(\"_actual\", \"_predict\")).merge(df_aux, on=[\"country\", \"brand_name\"], how=\"left\");\n",
    "    merged_data[\"start_month\"] = merged_data.groupby([\"country\", \"brand_name\"])[\"months_postgx\"].transform(\"min\");\n",
    "    merged_data = merged_data[merged_data[\"start_month\"] == 6].copy();\n",
    "    pe_results = (merged_data.groupby([\"country\", \"brand_name\", \"bucket\"]).apply(_compute_pe_phase1b).reset_index(name=\"PE\"));\n",
    "    bucket1 = pe_results[pe_results[\"bucket\"] == 1]; bucket2 = pe_results[pe_results[\"bucket\"] == 2];\n",
    "    n1 = len(bucket1); n2 = len(bucket2); score = 0;\n",
    "    if n1 > 0: score += (2 / n1) * bucket1[\"PE\"].sum();\n",
    "    if n2 > 0: score += (1 / n2) * bucket2[\"PE\"].sum();\n",
    "    return round(score, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8pJtzMnXR6N"
   },
   "source": [
    "Generate Predictions and Final Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSMxkDFnXZU9"
   },
   "source": [
    "Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KImrE3YYe9p"
   },
   "source": [
    "method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RWOUf2tbVs8",
    "outputId": "391c6dd8-f8f8-4aeb-fbc5-12b150fa56e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24832\\3520848473.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  pe_results = (merged.groupby([\"country\", \"brand_name\", \"bucket\"]).apply(_compute_pe_phase1a).reset_index(name=\"PE\"));\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final PE Score (Scenario 1, S1): 0.2118\n",
      "Final PE Score (Scenario 2, S2): 0.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24832\\3520848473.py:58: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  pe_results = (merged_data.groupby([\"country\", \"brand_name\", \"bucket\"]).apply(_compute_pe_phase1b).reset_index(name=\"PE\"));\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# use integer indices to split (keep original MultiIndex for aux) ---\n",
    "#TODO ; FIX THIS TO TRAIN TEST SPLIT\n",
    "val_frac = 0.2\n",
    "val_idx = np.random.choice(X1.shape[0], size=int(val_frac*X1.shape[0]), replace=False)\n",
    "train_idx = np.setdiff1d(np.arange(X1.shape[0]), val_idx)\n",
    "\n",
    "# Split original X1, X2 (not encoded arrays) for indexing\n",
    "X1_val_idx = X1.iloc[val_idx]\n",
    "Y1_val_idx = Y1.iloc[val_idx]\n",
    "X2_val_idx = X2.iloc[val_idx]\n",
    "Y2_val_idx = Y2.iloc[val_idx]\n",
    "\n",
    "# Also prepare aux data\n",
    "df_aux_val = pd.DataFrame({\n",
    "    'country': X1_val_idx.index.get_level_values('country'),\n",
    "    'brand_name': X1_val_idx.index.get_level_values('brand_name'),\n",
    "    'bucket': buckets[val_idx],\n",
    "    'avg_vol': X_base.loc[X1_val_idx.index, 'Avg_j'].values\n",
    "})\n",
    "\n",
    "# predict using the encoded arrays for validation ---\n",
    "preds_real_s1 = np.maximum(0, inverse_transform(m1.predict(X1_enc[val_idx])))\n",
    "preds_real_s2 = np.maximum(0, inverse_transform(m2.predict(X2_enc[val_idx])))\n",
    "\n",
    "# format long DataFrames correctly ---\n",
    "# Scenario 1\n",
    "df_actual_S1 = Y1_val_idx.stack().reset_index()\n",
    "df_actual_S1 = df_actual_S1.rename(columns={0: 'volume_actual', 'months_postgx': 'm_col'})\n",
    "df_actual_S1['months_postgx'] = df_actual_S1['m_col']\n",
    "\n",
    "df_pred_S1 = pd.DataFrame(preds_real_s1, index=Y1_val_idx.index, columns=Y1_val_idx.columns).stack().reset_index()\n",
    "df_pred_S1 = df_pred_S1.rename(columns={0: 'volume_predict', 'months_postgx': 'm_col'})\n",
    "df_pred_S1['months_postgx'] = df_pred_S1['m_col']\n",
    "\n",
    "# Scenario 2\n",
    "df_actual_S2 = Y2_val_idx.stack().reset_index()\n",
    "df_actual_S2 = df_actual_S2.rename(columns={0: 'volume_actual', 'months_postgx': 'm_col'})\n",
    "df_actual_S2['months_postgx'] = df_actual_S2['m_col']\n",
    "\n",
    "df_pred_S2 = pd.DataFrame(preds_real_s2, index=Y2_val_idx.index, columns=Y2_val_idx.columns).stack().reset_index()\n",
    "df_pred_S2 = df_pred_S2.rename(columns={0: 'volume_predict', 'months_postgx': 'm_col'})\n",
    "df_pred_S2['months_postgx'] = df_pred_S2['m_col']\n",
    "\n",
    "# compute PE Scores ---\n",
    "m1_score_final = compute_metric1(\n",
    "    df_actual_S1[['country', 'brand_name', 'months_postgx', 'volume_actual']],\n",
    "    df_pred_S1[['country', 'brand_name', 'months_postgx', 'volume_predict']],\n",
    "    df_aux_val\n",
    ")\n",
    "\n",
    "m2_score_final = compute_metric2(\n",
    "    df_actual_S2[['country', 'brand_name', 'months_postgx', 'volume_actual']],\n",
    "    df_pred_S2[['country', 'brand_name', 'months_postgx', 'volume_predict']],\n",
    "    df_aux_val\n",
    ")\n",
    "\n",
    "print(f\"Final PE Score (Scenario 1, S1): {m1_score_final}\")\n",
    "print(f\"Final PE Score (Scenario 2, S2): {m2_score_final}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ6pJhPgYhEj"
   },
   "source": [
    "method 2 - using metric_calculation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPcDaqgwUaHJ",
    "outputId": "4a3853ad-275f-4aec-a7c7-60b8af2b8475"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      5\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/')\n",
    "\n",
    "# import the metric functions\n",
    "from metric_calculation import compute_metric1, compute_metric2\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VALIDATION SPLIT\n",
    "# ------------------------------------------------------------\n",
    "val_frac = 0.2\n",
    "\n",
    "val_idx = np.random.choice(\n",
    "    X1.shape[0],\n",
    "    size=int(val_frac * X1.shape[0]),\n",
    "    replace=False\n",
    ")\n",
    "train_idx = np.setdiff1d(np.arange(X1.shape[0]), val_idx)\n",
    "\n",
    "# Extract original MultiIndexed data for validation\n",
    "X1_val = X1.iloc[val_idx]\n",
    "Y1_val = Y1.iloc[val_idx]\n",
    "\n",
    "X2_val = X2.iloc[val_idx]\n",
    "Y2_val = Y2.iloc[val_idx]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BUILD df_aux_val (bucket + avg volume for each brand)\n",
    "# ------------------------------------------------------------\n",
    "df_aux_val = pd.DataFrame({\n",
    "    'country': X1_val.index.get_level_values('country'),\n",
    "    'brand_name': X1_val.index.get_level_values('brand_name'),\n",
    "    'bucket': buckets[val_idx],\n",
    "    'avg_vol': X_base.loc[X1_val.index, 'Avg_j'].values\n",
    "})\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MAKE MODEL PREDICTIONS (inverse log transform)\n",
    "# ------------------------------------------------------------\n",
    "pred_s1 = np.maximum(0, inverse_transform(m1.predict(X1_enc[val_idx])))\n",
    "pred_s2 = np.maximum(0, inverse_transform(m2.predict(X2_enc[val_idx])))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Convert predictions + actuals to long format\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def to_long(actual_df, pred_array, value_name):\n",
    "    \"\"\"Convert wide-format (24 targets) to long format.\"\"\"\n",
    "    pred_df = pd.DataFrame(\n",
    "        pred_array,\n",
    "        index=actual_df.index,\n",
    "        columns=actual_df.columns\n",
    "    )\n",
    "\n",
    "    df_long = actual_df.stack().reset_index()\n",
    "    df_long = df_long.rename(columns={0: value_name})\n",
    "\n",
    "    pred_long = pred_df.stack().reset_index()\n",
    "    pred_long = pred_long.rename(columns={0: \"volume_predict\"})\n",
    "\n",
    "    return df_long, pred_long\n",
    "\n",
    "\n",
    "# S1\n",
    "df_actual_S1, df_pred_S1 = to_long(Y1_val, pred_s1, \"volume_actual\")\n",
    "\n",
    "# S2\n",
    "df_actual_S2, df_pred_S2 = to_long(Y2_val, pred_s2, \"volume_actual\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Select only required columns\n",
    "# ------------------------------------------------------------\n",
    "df_actual_S1 = df_actual_S1[[\"country\",\"brand_name\",\"months_postgx\",\"volume_actual\"]]\n",
    "df_pred_S1   = df_pred_S1[[\"country\",\"brand_name\",\"months_postgx\",\"volume_predict\"]]\n",
    "\n",
    "df_actual_S2 = df_actual_S2[[\"country\",\"brand_name\",\"months_postgx\",\"volume_actual\"]]\n",
    "df_pred_S2   = df_pred_S2[[\"country\",\"brand_name\",\"months_postgx\",\"volume_predict\"]]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# COMPUTE METRIC SCORES\n",
    "# ------------------------------------------------------------\n",
    "m1_score = compute_metric1(df_actual_S1, df_pred_S1, df_aux_val)\n",
    "m2_score = compute_metric2(df_actual_S2, df_pred_S2, df_aux_val)\n",
    "\n",
    "print(\"Final PE Score (Scenario 1 — Phase 1a):\", m1_score)\n",
    "print(\"Final PE Score (Scenario 2 — Phase 1b):\", m2_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tanbOHTmOvik"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # stratified index split (returns integer positions and MultiIndex rows) ----\n",
    "# # use integer indices for slicing X1_enc/X2_enc (NumPy) and MultiIndex for Y1/Y2\n",
    "# train_idx_int, val_idx_int = train_test_split(\n",
    "#     np.arange(len(X1)),\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=buckets\n",
    "# )\n",
    "# # MultiIndex rows for val\n",
    "# val_idx = X1.index[val_idx_int]\n",
    "\n",
    "# # build validation matrices ----\n",
    "# X1_val = X1_enc[val_idx_int]   # encoded features (NumPy)\n",
    "# X2_val = X2_enc[val_idx_int]\n",
    "# Y1_val = Y1.loc[val_idx]       # targets as DataFrame (MultiIndex rows)\n",
    "# Y2_val = Y2.loc[val_idx]\n",
    "\n",
    "# # build df_aux_val (country, brand_name, bucket, avg_vol) ----\n",
    "# df_aux_val = pd.DataFrame({\n",
    "#     'country': val_idx.get_level_values('country'),\n",
    "#     'brand_name': val_idx.get_level_values('brand_name'),\n",
    "#     'bucket': buckets[val_idx_int],\n",
    "#     'avg_vol': X_base.loc[val_idx, 'Avg_j'].values\n",
    "# })\n",
    "# # ensure types\n",
    "# df_aux_val['bucket'] = df_aux_val['bucket'].astype(int)\n",
    "\n",
    "# # predict and inverse-transform (ensure same shape) ----\n",
    "# preds_log_s1 = m1.predict(X1_val)   # shape: (n_val_rows, 24) expected\n",
    "# preds_log_s2 = m2.predict(X2_val)   # shape: (n_val_rows, 18) expected (6..23 -> 18 months)\n",
    "\n",
    "# preds_real_s1 = np.maximum(0, inverse_transform(preds_log_s1))\n",
    "# preds_real_s2 = np.maximum(0, inverse_transform(preds_log_s2))\n",
    "\n",
    "# # ---- 5. convert actuals -> long format (robust) ----\n",
    "# # After stack, reset_index() yields columns: ['country','brand_name','months_postgx', 0]\n",
    "# df_actual_S1 = Y1_val.stack().reset_index().rename(columns={0: 'volume_actual'})\n",
    "# # months_postgx is already an int column (the stacked column name). To be explicit:\n",
    "# df_actual_S1['months_postgx'] = df_actual_S1['months_postgx'].astype(int)\n",
    "\n",
    "# df_actual_S2 = Y2_val.stack().reset_index().rename(columns={0: 'volume_actual'})\n",
    "# df_actual_S2['months_postgx'] = df_actual_S2['months_postgx'].astype(int)\n",
    "\n",
    "# # convert preds -> long format (use same MultiIndex index/columns as Y_val) ----\n",
    "# df_pred_S1 = pd.DataFrame(preds_real_s1, index=Y1_val.index, columns=Y1_val.columns).stack().reset_index().rename(columns={0: 'volume_predict'})\n",
    "# df_pred_S1['months_postgx'] = df_pred_S1['months_postgx'].astype(int)\n",
    "\n",
    "# df_pred_S2 = pd.DataFrame(preds_real_s2, index=Y2_val.index, columns=Y2_val.columns).stack().reset_index().rename(columns={0: 'volume_predict'})\n",
    "# df_pred_S2['months_postgx'] = df_pred_S2['months_postgx'].astype(int)\n",
    "\n",
    "# # compute metrics (robust grouping -> Series -> reset_index) ----\n",
    "# def compute_metric1_stable(df_actual, df_pred, df_aux):\n",
    "#     merged = (\n",
    "#         df_actual.merge(df_pred, on=[\"country\",\"brand_name\",\"months_postgx\"], how=\"inner\", suffixes=(\"_actual\",\"_predict\"))\n",
    "#                  .merge(df_aux, on=[\"country\",\"brand_name\"], how=\"left\")\n",
    "#     )\n",
    "#     merged[\"start_month\"] = merged.groupby([\"country\",\"brand_name\"])[\"months_postgx\"].transform(\"min\")\n",
    "#     merged = merged[merged[\"start_month\"] == 0].copy()\n",
    "#     # per-group PE (series with MultiIndex)\n",
    "#     pe_series = merged.groupby([\"country\",\"brand_name\",\"bucket\"]).apply(lambda g: _compute_pe_phase1a(g))\n",
    "#     pe_results = pe_series.reset_index(name=\"PE\")\n",
    "#     bucket1 = pe_results[pe_results[\"bucket\"] == 1]\n",
    "#     bucket2 = pe_results[pe_results[\"bucket\"] == 2]\n",
    "#     n1 = bucket1[[\"country\",\"brand_name\"]].drop_duplicates().shape[0]\n",
    "#     n2 = bucket2[[\"country\",\"brand_name\"]].drop_duplicates().shape[0]\n",
    "#     score = 0.0\n",
    "#     if n1 > 0: score += (2.0 / n1) * bucket1[\"PE\"].sum()\n",
    "#     if n2 > 0: score += (1.0 / n2) * bucket2[\"PE\"].sum()\n",
    "#     return round(score, 4)\n",
    "\n",
    "# def compute_metric2_stable(df_actual, df_pred, df_aux):\n",
    "#     merged = (\n",
    "#         df_actual.merge(df_pred, on=[\"country\",\"brand_name\",\"months_postgx\"], how=\"inner\", suffixes=(\"_actual\",\"_predict\"))\n",
    "#                  .merge(df_aux, on=[\"country\",\"brand_name\"], how=\"left\")\n",
    "#     )\n",
    "#     merged[\"start_month\"] = merged.groupby([\"country\",\"brand_name\"])[\"months_postgx\"].transform(\"min\")\n",
    "#     merged = merged[merged[\"start_month\"] == 6].copy()\n",
    "#     pe_series = merged.groupby([\"country\",\"brand_name\",\"bucket\"]).apply(lambda g: _compute_pe_phase1b(g))\n",
    "#     pe_results = pe_series.reset_index(name=\"PE\")\n",
    "#     bucket1 = pe_results[pe_results[\"bucket\"] == 1]\n",
    "#     bucket2 = pe_results[pe_results[\"bucket\"] == 2]\n",
    "#     n1 = bucket1[[\"country\",\"brand_name\"]].drop_duplicates().shape[0]\n",
    "#     n2 = bucket2[[\"country\",\"brand_name\"]].drop_duplicates().shape[0]\n",
    "#     score = 0.0\n",
    "#     if n1 > 0: score += (2.0 / n1) * bucket1[\"PE\"].sum()\n",
    "#     if n2 > 0: score += (1.0 / n2) * bucket2[\"PE\"].sum()\n",
    "#     return round(score, 4)\n",
    "\n",
    "# m1_score_final = compute_metric1_stable(\n",
    "#     df_actual_S1[['country','brand_name','months_postgx','volume_actual']],\n",
    "#     df_pred_S1[['country','brand_name','months_postgx','volume_predict']],\n",
    "#     df_aux_val\n",
    "# )\n",
    "\n",
    "# m2_score_final = compute_metric2_stable(\n",
    "#     df_actual_S2[['country','brand_name','months_postgx','volume_actual']],\n",
    "#     df_pred_S2[['country','brand_name','months_postgx','volume_predict']],\n",
    "#     df_aux_val\n",
    "# )\n",
    "\n",
    "# print(f\"\\nFinal PE Score (Scenario 1, S1): {m1_score_final}\")\n",
    "# print(f\"Final PE Score (Scenario 2, S2): {m2_score_final}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIpTLEKsh2iq"
   },
   "source": [
    "0.2 has been an improvement ( from 0.6 using 400 num_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_qyRY9ZFeFKi"
   },
   "outputs": [],
   "source": [
    "# # to check encoding\n",
    "# # 1. Print UNENCODED Features (Pandas DataFrame)\n",
    "# # Shows the original categories and numeric values.\n",
    "# print(\"\\n[A] UNENCODED FEATURES (X1 - First 5 Rows):\")\n",
    "# print(X1.head())\n",
    "# print(\"-\" * 30)\n",
    "# print(f\"Original X1 Shape: {X1.shape}\")\n",
    "\n",
    "# # 2. Print ENCODED Features (NumPy Array)\n",
    "# # This shows the final numerical result fed into the XGBoost model.\n",
    "# print(\"\\n[B] ENCODED FEATURES (X1_enc - First 5 Rows of NumPy Array):\")\n",
    "# print(X1_enc[:5, :10]) # Print first 5 rows and first 10 columns\n",
    "# print(f\"Final X1_enc Shape: {X1_enc.shape}\")\n",
    "# print(\"-\" * 30)\n",
    "\n",
    "# # Optional: Print total number of features created (columns in the final array)\n",
    "# print(f\"Total Features Created After Encoding: {X1_enc.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
