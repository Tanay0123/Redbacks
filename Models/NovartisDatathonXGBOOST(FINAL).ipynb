{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSZDj6VNDDQ0",
    "outputId": "b668f523-5e06-4b86-ede2-148644ef9773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OK: (93744, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>month</th>\n",
       "      <th>months_postgx</th>\n",
       "      <th>volume</th>\n",
       "      <th>n_gxs</th>\n",
       "      <th>ther_area</th>\n",
       "      <th>hospital_rate</th>\n",
       "      <th>main_package</th>\n",
       "      <th>biological</th>\n",
       "      <th>small_molecule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COUNTRY_B6AE</td>\n",
       "      <td>BRAND_1C1E</td>\n",
       "      <td>Jul</td>\n",
       "      <td>-24</td>\n",
       "      <td>272594.3921</td>\n",
       "      <td>0</td>\n",
       "      <td>Muscoskeletal_Rheumatology_and_Osteology</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>PILL</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COUNTRY_B6AE</td>\n",
       "      <td>BRAND_1C1E</td>\n",
       "      <td>Aug</td>\n",
       "      <td>-23</td>\n",
       "      <td>351859.3103</td>\n",
       "      <td>0</td>\n",
       "      <td>Muscoskeletal_Rheumatology_and_Osteology</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>PILL</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COUNTRY_B6AE</td>\n",
       "      <td>BRAND_1C1E</td>\n",
       "      <td>Sep</td>\n",
       "      <td>-22</td>\n",
       "      <td>447953.4813</td>\n",
       "      <td>0</td>\n",
       "      <td>Muscoskeletal_Rheumatology_and_Osteology</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>PILL</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COUNTRY_B6AE</td>\n",
       "      <td>BRAND_1C1E</td>\n",
       "      <td>Oct</td>\n",
       "      <td>-21</td>\n",
       "      <td>411543.2924</td>\n",
       "      <td>0</td>\n",
       "      <td>Muscoskeletal_Rheumatology_and_Osteology</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>PILL</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COUNTRY_B6AE</td>\n",
       "      <td>BRAND_1C1E</td>\n",
       "      <td>Nov</td>\n",
       "      <td>-20</td>\n",
       "      <td>774594.4542</td>\n",
       "      <td>0</td>\n",
       "      <td>Muscoskeletal_Rheumatology_and_Osteology</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>PILL</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country  brand_name month  months_postgx       volume  n_gxs  \\\n",
       "0  COUNTRY_B6AE  BRAND_1C1E   Jul            -24  272594.3921      0   \n",
       "1  COUNTRY_B6AE  BRAND_1C1E   Aug            -23  351859.3103      0   \n",
       "2  COUNTRY_B6AE  BRAND_1C1E   Sep            -22  447953.4813      0   \n",
       "3  COUNTRY_B6AE  BRAND_1C1E   Oct            -21  411543.2924      0   \n",
       "4  COUNTRY_B6AE  BRAND_1C1E   Nov            -20  774594.4542      0   \n",
       "\n",
       "                                  ther_area  hospital_rate main_package  \\\n",
       "0  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
       "1  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
       "2  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
       "3  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
       "4  Muscoskeletal_Rheumatology_and_Osteology       0.002088         PILL   \n",
       "\n",
       "   biological  small_molecule  \n",
       "0       False            True  \n",
       "1       False            True  \n",
       "2       False            True  \n",
       "3       False            True  \n",
       "4       False            True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"Data/TRAIN/merged_cleaned_train_data.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "print(\"Loaded OK:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YYyAB32W9wm"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yZjHfXToW_lV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.multioutput import MultiOutputRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD3H-PDrXFPn"
   },
   "source": [
    "Best parameters (Taken by randomsearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A6qh9PiiXCP4"
   },
   "outputs": [],
   "source": [
    "FINAL_XGB_PARAMS = { # needs higher n_estimators to find subtle patterns and avoid underfitting\n",
    "    'n_estimators': 2000, # need to be higher like 2000\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'reg_lambda': 6.0, #FIXME\n",
    "}\n",
    "\n",
    "\n",
    "# separated parameter for s2\n",
    "FINAL_XGB_PARAMS_s2 = { #converges quickly due to the highly predictive features so needs lower n_estimators to prevent overfitting\n",
    "    'n_estimators': 600, #  400-600 would be good\n",
    "    'learning_rate': 0.04, # thinking of changing this to a lower value\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 10,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'reg_lambda': 6.0, #FIXME\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCTQMtJoSKft"
   },
   "source": [
    "Path files and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "p0V21xHDSJfH"
   },
   "outputs": [],
   "source": [
    "DRIVE_TRAIN_PATH = 'Data/TRAIN/merged_cleaned_train_data.csv'\n",
    "TEST_FILE_NAME = 'Data/TRAIN/merged_cleaned_train_data.csv'\n",
    "KEYS_STATIC = ['country', 'brand_name']\n",
    "static_cols = ['ther_area', 'main_package', 'biological', 'small_molecule', 'hospital_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J14yaUbOO1Se",
    "outputId": "1f11ac81-c04c-4b86-e132-b154d80bbee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training on full dataset\n",
      "Final Tuned Models Trained Successfully.\n"
     ]
    }
   ],
   "source": [
    "# helper functions\n",
    "def log_transform(df): return np.log1p(df) # shrinks high volume data and stretches the small ones to make the data easier to learn from\n",
    "def inverse_transform(arr): return np.expm1(arr) # bias correction when predictions were too low\n",
    "\n",
    "# load data and prep\n",
    "df = pd.read_csv(DRIVE_TRAIN_PATH)\n",
    "\n",
    "#drops any redundant columns\n",
    "if 'Unnamed: 0' in df.columns: df = df.drop(columns=['Unnamed: 0'])\n",
    "# replaces missing values with mean value to preserve row while maintaining the overall distribution of the variable\n",
    "df['hospital_rate'] = df['hospital_rate'].fillna(df['hospital_rate'].mean())\n",
    "\n",
    "# feature engineering\n",
    "df_pre = df[(df['months_postgx'] >= -12) & (df['months_postgx'] <= -1)] # uses 12 months immediately preceding generic entry\n",
    "avg_j = df_pre.groupby(KEYS_STATIC)['volume'].mean().reset_index(name='Avg_j') # used because PE is defined as the mean of the last 12 months before generic entry\n",
    "df_static = df.drop_duplicates(subset=KEYS_STATIC)[KEYS_STATIC + static_cols]\n",
    "X_base = pd.merge(df_static, avg_j, on=KEYS_STATIC, how='inner').set_index(KEYS_STATIC) #for scenario 1 prediction (uninformed features)\n",
    "\n",
    "# X1, Y1 (M0..M23)\n",
    "df_Y1 = df[(df['months_postgx'] >= 0) & (df['months_postgx'] <= 23)] #filters raw data to only include 24 months after generic entry\n",
    "Y1 = df_Y1.pivot_table(index=KEYS_STATIC, columns='months_postgx', values='volume').fillna(np.nan) # target matrix for y1\n",
    "X1 = pd.merge(X_base, Y1, left_index=True, right_index=True, how='inner').iloc[:, :-24]\n",
    "Y1 = Y1.loc[X1.index] # created final target natrix\n",
    "\n",
    "# X2, Y2 (M6..M23 with M0-M5 features) - Scenario 2 (Informed)\n",
    "\n",
    "# feature engineering for scenario 2\n",
    "df_M0_M5 = df[(df['months_postgx'] >= 0) & (df['months_postgx'] <= 5)].pivot_table(index=KEYS_STATIC, columns='months_postgx', values='volume').fillna(np.nan) # filters data to isolate actual volumes\n",
    "M0_M5_feats = pd.concat([df_M0_M5[0].rename('Vol_M0'), df_M0_M5[5].rename('Vol_M5'), (df_M0_M5[0] - df_M0_M5[5]).rename('Erosion_M0_M5')], axis=1) # calculates initial erosion features months 0-6 post entry volume\n",
    "X2 = pd.merge(X_base, M0_M5_feats, left_index=True, right_index=True, how='inner') # includes all input information necessary for the informed forecast from month 6 onwards\n",
    "\n",
    "df_Y2 = df[(df['months_postgx'] >= 6) & (df['months_postgx'] <= 23)] # 18 month prediction\n",
    "Y2 = df_Y2.pivot_table(index=KEYS_STATIC, columns='months_postgx', values='volume').fillna(np.nan)\n",
    "Y2 = Y2.loc[X2.index] # created final target matrix\n",
    "\n",
    "# encoding to prep data for XGBoost\n",
    "X1['country_feat'] = X1.index.get_level_values('country')\n",
    "X2['country_feat'] = X2.index.get_level_values('country')\n",
    "cat_cols = X1.select_dtypes(include=['object']).columns.tolist()\n",
    "preprocessor = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)], remainder='passthrough') # applied one hot encoding to use XGBoost efficiently\n",
    "# fit and transform( maximizing data utilization)\n",
    "X1_enc = preprocessor.fit_transform(X1).astype(float)\n",
    "X2_enc = preprocessor.transform(X2).astype(float)\n",
    "\n",
    "# Full Data Buckets (for training weights)\n",
    "Y1_norm = Y1.div(X_base['Avg_j'], axis=0) # calculate normalized volume\n",
    "buckets = np.where(Y1_norm.mean(axis=1) <= 0.25, 1, 2) # if bucket labelled 1 = high erosion\n",
    "weights_full = np.where(buckets == 1, 2.0, 1.0) # this forces model to treat high erosion drugs error twice as costly\n",
    "\n",
    "\n",
    "# Model Training\n",
    "print(\"\\n training on full dataset\")\n",
    "Y1_log = log_transform(Y1)\n",
    "Y2_log = log_transform(Y2)\n",
    "\n",
    "# Model 1 (S1)\n",
    "m1 = MultiOutputRegressor(xgb.XGBRegressor(**FINAL_XGB_PARAMS))\n",
    "m1.fit(X1_enc, Y1_log.values, sample_weight=weights_full) \n",
    "\n",
    "# Model 2 (S2)\n",
    "m2 = MultiOutputRegressor(xgb.XGBRegressor(**FINAL_XGB_PARAMS_s2))\n",
    "m2.fit(X2_enc, Y2_log.values, sample_weight=weights_full)\n",
    "print(\"Final Tuned Models Trained Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# leasrning curve func\n",
    "def plot_learning_curve_single_output(model, X, y, title=\"Learning Curve\", cv=5, scoring=\"neg_mean_absolute_error\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 8),\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    train_std  = np.std(train_scores, axis=1)\n",
    "    val_std  = np.std(val_scores, axis=1)\n",
    "\n",
    "    plt.plot(train_sizes, train_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, val_mean, label=\"Validation score\")\n",
    "\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Size\")\n",
    "    plt.ylabel(\"Score (MAE, neg)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCENARIO 1\n",
    "# inspecting the first month which is month 0 in scenario 1 (since can only plot a single month for learning curve)\n",
    "y_s1 = Y1_log.values[:, 0]\n",
    "\n",
    "base_model_s1 = xgb.XGBRegressor(**FINAL_XGB_PARAMS)\n",
    "\n",
    "plot_learning_curve_single_output(\n",
    "    base_model_s1,\n",
    "    X1_enc,\n",
    "    y_s1,\n",
    "    title=\"S1 learning curve on month 0\"\n",
    ")\n",
    "\n",
    "# SCENARIO 2\n",
    "# plottin gonly month 6 \n",
    "y_s2 = Y2_log.values[:, 0]\n",
    "\n",
    "base_model_s2 = xgb.XGBRegressor(**FINAL_XGB_PARAMS_s2)\n",
    "\n",
    "plot_learning_curve_single_output(\n",
    "    base_model_s2,\n",
    "    X2_enc,\n",
    "    y_s2,\n",
    "    title=\"s2 learning curve on month 6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "u02tj4ZUf6ce",
    "outputId": "5e886636-bf66-4547-98e0-59143f950871"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m X_test_S2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_feat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_test_S2\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_level_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# encoding\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m X_test_S1_enc \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(X_test_S1)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     32\u001b[0m X_test_S2_enc \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(X_test_S2)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1076\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1074\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[1;32m-> 1076\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_func_on_transformers(\n\u001b[0;32m   1077\u001b[0m     X,\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1079\u001b[0m     _transform_one,\n\u001b[0;32m   1080\u001b[0m     column_as_labels\u001b[38;5;241m=\u001b[39mfit_dataframe_and_transform_dataframe,\n\u001b[0;32m   1081\u001b[0m     routed_params\u001b[38;5;241m=\u001b[39mrouted_params,\n\u001b[0;32m   1082\u001b[0m )\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:885\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[1;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[0;32m    873\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    874\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    875\u001b[0m             delayed(func)(\n\u001b[0;32m    876\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    882\u001b[0m             )\n\u001b[0;32m    883\u001b[0m         )\n\u001b[1;32m--> 885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(jobs)\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1290\u001b[0m, in \u001b[0;36m_transform_one\u001b[1;34m(transformer, X, y, weight, params)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_one\u001b[39m(transformer, X, y, weight, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call transform and apply weight to output.\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m \n\u001b[0;32m   1271\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;124;03m        This should be of the form ``process_routing()[\"step_name\"]``.\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1290\u001b[0m     res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mtransform(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mtransform)\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1024\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1023\u001b[0m }\n\u001b[1;32m-> 1024\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m   1025\u001b[0m     X,\n\u001b[0;32m   1026\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m   1027\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1028\u001b[0m     warn_on_unknown\u001b[38;5;241m=\u001b[39mwarn_on_unknown,\n\u001b[0;32m   1029\u001b[0m )\n\u001b[0;32m   1031\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:194\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    188\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m     ignore_category_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    193\u001b[0m ):\n\u001b[1;32m--> 194\u001b[0m     X_list, n_samples, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X(\n\u001b[0;32m    195\u001b[0m         X, force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:61\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[1;34m(self, X, force_all_finite)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m     60\u001b[0m     Xi \u001b[38;5;241m=\u001b[39m _safe_indexing(X, indices\u001b[38;5;241m=\u001b[39mi, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m     Xi \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m     62\u001b[0m         Xi, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force_all_finite\u001b[38;5;241m=\u001b[39mneeds_validation\n\u001b[0;32m     63\u001b[0m     )\n\u001b[0;32m     64\u001b[0m     X_columns\u001b[38;5;241m.\u001b[39mappend(Xi)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_columns, n_samples, n_features\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1091\u001b[0m         )\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Submission CSV\n",
    "try:\n",
    "    df_test = pd.read_csv(TEST_FILE_NAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{TEST_FILE_NAME}' not found.\")\n",
    "    raise\n",
    "\n",
    "# initial cleaning (Test)\n",
    "df_test['n_gxs'] = df_test['n_gxs'].fillna(0.0)\n",
    "df_test['hospital_rate'] = df_test['hospital_rate'].fillna(df_test['hospital_rate'].mean())\n",
    "\n",
    "# test Feature Engineering (perfectly match train FE)\n",
    "df_pre_t = df_test[(df_test['months_postgx'] >= -12) & (df_test['months_postgx'] <= -1)]\n",
    "avg_j_t = df_pre_t.groupby(KEYS_STATIC)['volume'].mean().reset_index(name='Avg_j')\n",
    "df_static_t = df_test.drop_duplicates(subset=KEYS_STATIC)[KEYS_STATIC + static_cols]\n",
    "X_base_t = pd.merge(df_static_t, avg_j_t, on=KEYS_STATIC, how='inner').set_index(KEYS_STATIC)\n",
    "df_M0_M5_t = df_test[(df_test['months_postgx'] >= 0) & (df_test['months_postgx'] <= 5)].pivot_table(index=KEYS_STATIC, columns='months_postgx', values='volume').fillna(np.nan)\n",
    "M0_M5_feats_t = pd.concat([df_M0_M5_t[0].rename('Vol_M0'), df_M0_M5_t[5].rename('Vol_M5'), (df_M0_M5_t[0] - df_M0_M5_t[5]).rename('Erosion_M0_M5')], axis=1)\n",
    "\n",
    "# split Test drugs\n",
    "s2_drugs = M0_M5_feats_t.dropna().index\n",
    "s1_drugs = X_base_t.index.difference(s2_drugs)\n",
    "\n",
    "# prepare final prediction matrices\n",
    "X_test_S1 = X_base_t.loc[s1_drugs].copy()\n",
    "X_test_S1['country_feat'] = X_test_S1.index.get_level_values('country')\n",
    "X_test_S2 = pd.merge(X_base_t, M0_M5_feats_t, left_index=True, right_index=True, how='inner').loc[s2_drugs]\n",
    "X_test_S2['country_feat'] = X_test_S2.index.get_level_values('country')\n",
    "\n",
    "# encoding\n",
    "X_test_S1_enc = preprocessor.transform(X_test_S1).astype(float)\n",
    "X_test_S2_enc = preprocessor.transform(X_test_S2).astype(float)\n",
    "\n",
    "# predict\n",
    "p_s1_log = m1.predict(X_test_S1_enc)\n",
    "p_s2_log = m2.predict(X_test_S2_enc)\n",
    "\n",
    "p_s1_final = np.maximum(0, inverse_transform(p_s1_log))\n",
    "p_s2_final = np.maximum(0, inverse_transform(p_s2_log))\n",
    "\n",
    "# Combine and Save\n",
    "df_p1 = pd.DataFrame(p_s1_final, index=X_test_S1.index, columns=[f'Volume_M{i}' for i in range(24)])\n",
    "df_p2 = pd.DataFrame(p_s2_final, index=X_test_S2.index, columns=[f'Volume_M{i}' for i in range(6, 24)])\n",
    "\n",
    "p1_long = df_p1.stack().reset_index()\n",
    "p1_long.columns = ['country', 'brand_name', 'm_col', 'volume']\n",
    "p1_long['months_postgx'] = p1_long['m_col'].str.replace('Volume_M', '').astype(int)\n",
    "\n",
    "p2_long = df_p2.stack().reset_index()\n",
    "p2_long.columns = ['country', 'brand_name', 'm_col', 'volume']\n",
    "p2_long['months_postgx'] = p2_long['m_col'].str.replace('Volume_M', '').astype(int)\n",
    "\n",
    "final_submission = pd.concat([p1_long[['country', 'brand_name', 'months_postgx', 'volume']], p2_long[['country', 'brand_name', 'months_postgx', 'volume']]])\n",
    "\n",
    "filename = 'final.csv'\n",
    "final_submission.to_csv(filename, index=False)\n",
    "# files.download(filename)\n",
    "print(f\"Submission Complete! Final CSV '{filename}' created with {len(final_submission)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1et6rQUDRgad"
   },
   "source": [
    "Metrics calculation (From Novartis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfELVPo3XQi4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# successfully defined:\n",
    "# X1_val, Y1_val, X2_val, Y2_val (Validation feature/target sets)\n",
    "# m1, m2 (Trained XGBoost models)\n",
    "# df_aux_val (Auxiliary data with bucket/avg_vol)\n",
    "# log_transform, inverse_transform (Helper functions)\n",
    "\n",
    "def _compute_pe_phase1a(group: pd.DataFrame) -> float:\n",
    "\n",
    "    avg_vol = group[\"avg_vol\"].iloc[0];\n",
    "    if avg_vol == 0 or np.isnan(avg_vol): return np.nan\n",
    "    def sum_abs_diff(month_start: int, month_end: int) -> float:\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)];\n",
    "        return (subset[\"volume_actual\"] - subset[\"volume_predict\"]).abs().sum()\n",
    "    def abs_sum_diff(month_start: int, month_end: int) -> float:\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)];\n",
    "        return abs(subset[\"volume_actual\"].sum() - subset[\"volume_predict\"].sum())\n",
    "    term1 = 0.2 * sum_abs_diff(0, 23) / (24 * avg_vol);\n",
    "    term2 = 0.5 * abs_sum_diff(0, 5) / (6 * avg_vol);\n",
    "    term3 = 0.2 * abs_sum_diff(6, 11) / (6 * avg_vol);\n",
    "    term4 = 0.1 * abs_sum_diff(12, 23) / (12 * avg_vol);\n",
    "    return term1 + term2 + term3 + term4\n",
    "\n",
    "def compute_metric1(df_actual: pd.DataFrame, df_pred: pd.DataFrame, df_aux: pd.DataFrame) -> float:\n",
    "\n",
    "    merged = df_actual.merge(df_pred, on=[\"country\", \"brand_name\", \"months_postgx\"], how=\"inner\", suffixes=(\"_actual\", \"_predict\")).merge(df_aux, on=[\"country\", \"brand_name\"], how=\"left\");\n",
    "    merged[\"start_month\"] = merged.groupby([\"country\", \"brand_name\"])[\"months_postgx\"].transform(\"min\");\n",
    "    merged = merged[merged[\"start_month\"] == 0].copy();\n",
    "    pe_results = (merged.groupby([\"country\", \"brand_name\", \"bucket\"]).apply(_compute_pe_phase1a).reset_index(name=\"PE\"));\n",
    "    bucket1 = pe_results[pe_results[\"bucket\"] == 1]; bucket2 = pe_results[pe_results[\"bucket\"] == 2];\n",
    "    n1 = len(bucket1); n2 = len(bucket2); score = 0;\n",
    "    if n1 > 0: score += (2 / n1) * bucket1[\"PE\"].sum();\n",
    "    if n2 > 0: score += (1 / n2) * bucket2[\"PE\"].sum();\n",
    "    return round(score, 4)\n",
    "\n",
    "def _compute_pe_phase1b(group: pd.DataFrame) -> float:\n",
    "\n",
    "    avg_vol = group[\"avg_vol\"].iloc[0];\n",
    "    if avg_vol == 0 or np.isnan(avg_vol): return np.nan\n",
    "    def sum_abs_diff(month_start: int, month_end: int) -> float:\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)];\n",
    "        return (subset[\"volume_actual\"] - subset[\"volume_predict\"]).abs().sum()\n",
    "    def abs_sum_diff(month_start: int, month_end: int) -> float:\n",
    "        subset = group[(group[\"months_postgx\"] >= month_start) & (group[\"months_postgx\"] <= month_end)];\n",
    "        return abs(subset[\"volume_actual\"].sum() - subset[\"volume_predict\"].sum())\n",
    "    term1 = 0.2 * sum_abs_diff(6, 23) / (18 * avg_vol);\n",
    "    term2 = 0.5 * abs_sum_diff(6, 11) / (6 * avg_vol);\n",
    "    term3 = 0.3 * abs_sum_diff(12, 23) / (12 * avg_vol);\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def compute_metric2(df_actual: pd.DataFrame, df_pred: pd.DataFrame, df_aux: pd.DataFrame) -> float:\n",
    "\n",
    "    merged_data = df_actual.merge(df_pred, on=[\"country\", \"brand_name\", \"months_postgx\"], how=\"inner\", suffixes=(\"_actual\", \"_predict\")).merge(df_aux, on=[\"country\", \"brand_name\"], how=\"left\");\n",
    "    merged_data[\"start_month\"] = merged_data.groupby([\"country\", \"brand_name\"])[\"months_postgx\"].transform(\"min\");\n",
    "    merged_data = merged_data[merged_data[\"start_month\"] == 6].copy();\n",
    "    pe_results = (merged_data.groupby([\"country\", \"brand_name\", \"bucket\"]).apply(_compute_pe_phase1b).reset_index(name=\"PE\"));\n",
    "    bucket1 = pe_results[pe_results[\"bucket\"] == 1]; bucket2 = pe_results[pe_results[\"bucket\"] == 2];\n",
    "    n1 = len(bucket1); n2 = len(bucket2); score = 0;\n",
    "    if n1 > 0: score += (2 / n1) * bucket1[\"PE\"].sum();\n",
    "    if n2 > 0: score += (1 / n2) * bucket2[\"PE\"].sum();\n",
    "    return round(score, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8pJtzMnXR6N"
   },
   "source": [
    "Generate Predictions and Final Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSMxkDFnXZU9"
   },
   "source": [
    "Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KImrE3YYe9p"
   },
   "source": [
    "method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RWOUf2tbVs8",
    "outputId": "391c6dd8-f8f8-4aeb-fbc5-12b150fa56e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# use integer indices to split (keep original MultiIndex for aux) ---\n",
    "#TODO ; FIX THIS TO TRAIN TEST SPLIT\n",
    "val_frac = 0.2\n",
    "val_idx = np.random.choice(X1.shape[0], size=int(val_frac*X1.shape[0]), replace=False)\n",
    "train_idx = np.setdiff1d(np.arange(X1.shape[0]), val_idx)\n",
    "\n",
    "# Split original X1, X2 (not encoded arrays) for indexing\n",
    "X1_val_idx = X1.iloc[val_idx]\n",
    "Y1_val_idx = Y1.iloc[val_idx]\n",
    "X2_val_idx = X2.iloc[val_idx]\n",
    "Y2_val_idx = Y2.iloc[val_idx]\n",
    "\n",
    "# Also prepare aux data\n",
    "df_aux_val = pd.DataFrame({\n",
    "    'country': X1_val_idx.index.get_level_values('country'),\n",
    "    'brand_name': X1_val_idx.index.get_level_values('brand_name'),\n",
    "    'bucket': buckets[val_idx],\n",
    "    'avg_vol': X_base.loc[X1_val_idx.index, 'Avg_j'].values\n",
    "})\n",
    "\n",
    "# predict using the encoded arrays for validation ---\n",
    "preds_real_s1 = np.maximum(0, inverse_transform(m1.predict(X1_enc[val_idx])))\n",
    "preds_real_s2 = np.maximum(0, inverse_transform(m2.predict(X2_enc[val_idx])))\n",
    "\n",
    "# format long DataFrames correctly ---\n",
    "# Scenario 1\n",
    "df_actual_S1 = Y1_val_idx.stack().reset_index()\n",
    "df_actual_S1 = df_actual_S1.rename(columns={0: 'volume_actual', 'months_postgx': 'm_col'})\n",
    "df_actual_S1['months_postgx'] = df_actual_S1['m_col']\n",
    "\n",
    "df_pred_S1 = pd.DataFrame(preds_real_s1, index=Y1_val_idx.index, columns=Y1_val_idx.columns).stack().reset_index()\n",
    "df_pred_S1 = df_pred_S1.rename(columns={0: 'volume_predict', 'months_postgx': 'm_col'})\n",
    "df_pred_S1['months_postgx'] = df_pred_S1['m_col']\n",
    "\n",
    "# Scenario 2\n",
    "df_actual_S2 = Y2_val_idx.stack().reset_index()\n",
    "df_actual_S2 = df_actual_S2.rename(columns={0: 'volume_actual', 'months_postgx': 'm_col'})\n",
    "df_actual_S2['months_postgx'] = df_actual_S2['m_col']\n",
    "\n",
    "df_pred_S2 = pd.DataFrame(preds_real_s2, index=Y2_val_idx.index, columns=Y2_val_idx.columns).stack().reset_index()\n",
    "df_pred_S2 = df_pred_S2.rename(columns={0: 'volume_predict', 'months_postgx': 'm_col'})\n",
    "df_pred_S2['months_postgx'] = df_pred_S2['m_col']\n",
    "\n",
    "# compute PE Scores ---\n",
    "m1_score_final = compute_metric1(\n",
    "    df_actual_S1[['country', 'brand_name', 'months_postgx', 'volume_actual']],\n",
    "    df_pred_S1[['country', 'brand_name', 'months_postgx', 'volume_predict']],\n",
    "    df_aux_val\n",
    ")\n",
    "\n",
    "m2_score_final = compute_metric2(\n",
    "    df_actual_S2[['country', 'brand_name', 'months_postgx', 'volume_actual']],\n",
    "    df_pred_S2[['country', 'brand_name', 'months_postgx', 'volume_predict']],\n",
    "    df_aux_val\n",
    ")\n",
    "\n",
    "print(f\"Final PE Score (Scenario 1, S1): {m1_score_final}\")\n",
    "print(f\"Final PE Score (Scenario 2, S2): {m2_score_final}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ6pJhPgYhEj"
   },
   "source": [
    "method 2 - using metric_calculation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPcDaqgwUaHJ",
    "outputId": "4a3853ad-275f-4aec-a7c7-60b8af2b8475"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import the metric functions\n",
    "from metric_calculation import compute_metric1, compute_metric2\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# VALIDATION SPLIT\n",
    "# ------------------------------------------------------------\n",
    "val_frac = 0.2\n",
    "\n",
    "val_idx = np.random.choice(\n",
    "    X1.shape[0],\n",
    "    size=int(val_frac * X1.shape[0]),\n",
    "    replace=False\n",
    ")\n",
    "train_idx = np.setdiff1d(np.arange(X1.shape[0]), val_idx)\n",
    "\n",
    "# Extract original MultiIndexed data for validation\n",
    "X1_val = X1.iloc[val_idx]\n",
    "Y1_val = Y1.iloc[val_idx]\n",
    "\n",
    "X2_val = X2.iloc[val_idx]\n",
    "Y2_val = Y2.iloc[val_idx]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BUILD df_aux_val (bucket + avg volume for each brand)\n",
    "# ------------------------------------------------------------\n",
    "df_aux_val = pd.DataFrame({\n",
    "    'country': X1_val.index.get_level_values('country'),\n",
    "    'brand_name': X1_val.index.get_level_values('brand_name'),\n",
    "    'bucket': buckets[val_idx],\n",
    "    'avg_vol': X_base.loc[X1_val.index, 'Avg_j'].values\n",
    "})\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MAKE MODEL PREDICTIONS (inverse log transform)\n",
    "# ------------------------------------------------------------\n",
    "pred_s1 = np.maximum(0, inverse_transform(m1.predict(X1_enc[val_idx])))\n",
    "pred_s2 = np.maximum(0, inverse_transform(m2.predict(X2_enc[val_idx])))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Convert predictions + actuals to long format\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def to_long(actual_df, pred_array, value_name):\n",
    "    \"\"\"Convert wide-format (24 targets) to long format.\"\"\"\n",
    "    pred_df = pd.DataFrame(\n",
    "        pred_array,\n",
    "        index=actual_df.index,\n",
    "        columns=actual_df.columns\n",
    "    )\n",
    "\n",
    "    df_long = actual_df.stack().reset_index()\n",
    "    df_long = df_long.rename(columns={0: value_name})\n",
    "\n",
    "    pred_long = pred_df.stack().reset_index()\n",
    "    pred_long = pred_long.rename(columns={0: \"volume_predict\"})\n",
    "\n",
    "    return df_long, pred_long\n",
    "\n",
    "\n",
    "# S1\n",
    "df_actual_S1, df_pred_S1 = to_long(Y1_val, pred_s1, \"volume_actual\")\n",
    "\n",
    "# S2\n",
    "df_actual_S2, df_pred_S2 = to_long(Y2_val, pred_s2, \"volume_actual\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Select only required columns\n",
    "# ------------------------------------------------------------\n",
    "df_actual_S1 = df_actual_S1[[\"country\",\"brand_name\",\"months_postgx\",\"volume_actual\"]]\n",
    "df_pred_S1   = df_pred_S1[[\"country\",\"brand_name\",\"months_postgx\",\"volume_predict\"]]\n",
    "\n",
    "df_actual_S2 = df_actual_S2[[\"country\",\"brand_name\",\"months_postgx\",\"volume_actual\"]]\n",
    "df_pred_S2   = df_pred_S2[[\"country\",\"brand_name\",\"months_postgx\",\"volume_predict\"]]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# COMPUTE METRIC SCORES\n",
    "# ------------------------------------------------------------\n",
    "m1_score = compute_metric1(df_actual_S1, df_pred_S1, df_aux_val)\n",
    "m2_score = compute_metric2(df_actual_S2, df_pred_S2, df_aux_val)\n",
    "\n",
    "print(\"Final PE Score (Scenario 1  Phase 1a):\", m1_score)\n",
    "print(\"Final PE Score (Scenario 2  Phase 1b):\", m2_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tanbOHTmOvik"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # stratified index split (returns integer positions and MultiIndex rows) ----\n",
    "# # use integer indices for slicing X1_enc/X2_enc (NumPy) and MultiIndex for Y1/Y2\n",
    "# train_idx_int, val_idx_int = train_test_split(\n",
    "#     np.arange(len(X1)),\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=buckets\n",
    "# )\n",
    "# # MultiIndex rows for val\n",
    "# val_idx = X1.index[val_idx_int]\n",
    "\n",
    "# # build validation matrices ----\n",
    "# X1_val = X1_enc[val_idx_int]   # encoded features (NumPy)\n",
    "# X2_val = X2_enc[val_idx_int]\n",
    "# Y1_val = Y1.loc[val_idx]       # targets as DataFrame (MultiIndex rows)\n",
    "# Y2_val = Y2.loc[val_idx]\n",
    "\n",
    "# # build df_aux_val (country, brand_name, bucket, avg_vol) ----\n",
    "# df_aux_val = pd.DataFrame({\n",
    "#     'country': val_idx.get_level_values('country'),\n",
    "#     'brand_name': val_idx.get_level_values('brand_name'),\n",
    "#     'bucket': buckets[val_idx_int],\n",
    "#     'avg_vol': X_base.loc[val_idx, 'Avg_j'].values\n",
    "# })\n",
    "# # ensure types\n",
    "# df_aux_val['bucket'] = df_aux_val['bucket'].astype(int)\n",
    "\n",
    "# # predict and inverse-transform (ensure same shape) ----\n",
    "# preds_log_s1 = m1.predict(X1_val)   # shape: (n_val_rows, 24) expected\n",
    "# preds_log_s2 = m2.predict(X2_val)   # shape: (n_val_rows, 18) expected (6..23 -> 18 months)\n",
    "\n",
    "# preds_real_s1 = np.maximum(0, inverse_transform(preds_log_s1))\n",
    "# preds_real_s2 = np.maximum(0, inverse_transform(preds_log_s2))\n",
    "\n",
    "# # ---- 5. convert actuals -> long format (robust) ----\n",
    "# # After stack, reset_index() yields columns: ['country','brand_name','months_postgx', 0]\n",
    "# df_actual_S1 = Y1_val.stack().reset_index().rename(columns={0: 'volume_actual'})\n",
    "# # months_postgx is already an int column (the stacked column name). To be explicit:\n",
    "# df_actual_S1['months_postgx'] = df_actual_S1['months_postgx'].astype(int)\n",
    "\n",
    "# df_actual_S2 = Y2_val.stack().reset_index().rename(columns={0: 'volume_actual'})\n",
    "# df_actual_S2['months_postgx'] = df_actual_S2['months_postgx'].astype(int)\n",
    "\n",
    "# # convert preds -> long format (use same MultiIndex index/columns as Y_val) ----\n",
    "# df_pred_S1 = pd.DataFrame(preds_real_s1, index=Y1_val.index, columns=Y1_val.columns).stack().reset_index().rename(columns={0: 'volume_predict'})\n",
    "# df_pred_S1['months_postgx'] = df_pred_S1['months_postgx'].astype(int)\n",
    "\n",
    "# df_pred_S2 = pd.DataFrame(preds_real_s2, index=Y2_val.index, columns=Y2_val.columns).stack().reset_index().rename(columns={0: 'volume_predict'})\n",
    "# df_pred_S2['months_postgx'] = df_pred_S2['months_postgx'].astype(int)\n",
    "\n",
    "# # compute metrics (robust grouping -> Series -> reset_index) ----\n",
    "# def compute_metric1_stable(df_actual, df_pred, df_aux):\n",
    "#     merged = (\n",
    "#         df_actual.merge(df_pred, on=[\"country\",\"brand_name\",\"months_postgx\"], how=\"inner\", suffixes=(\"_actual\",\"_predict\"))\n",
    "#                  .merge(df_aux, on=[\"country\",\"brand_name\"], how=\"left\")\n",
    "#     )\n",
    "#     merged[\"start_month\"] = merged.groupby([\"country\",\"brand_name\"])[\"months_postgx\"].transform(\"min\")\n",
    "#     merged = merged[merged[\"start_month\"] == 0].copy()\n",
    "#     # per-group PE (series with MultiIndex)\n",
    "#     pe_series = merged.groupby([\"country\",\"brand_name\",\"bucket\"]).apply(lambda g: _compute_pe_phase1a(g))\n",
    "#     pe_results = pe_series.reset_index(name=\"PE\")\n",
    "#     bucket1 = pe_results[pe_results[\"bucket\"] == 1]\n",
    "#     bucket2 = pe_results[pe_results[\"bucket\"] == 2]\n",
    "#     n1 = bucket1[[\"country\",\"brand_name\"]].drop_duplicates().shape[0]\n",
    "#     n2 = bucket2[[\"country\",\"brand_name\"]].drop_duplicates().shape[0]\n",
    "#     score = 0.0\n",
    "#     if n1 > 0: score += (2.0 / n1) * bucket1[\"PE\"].sum()\n",
    "#     if n2 > 0: score += (1.0 / n2) * bucket2[\"PE\"].sum()\n",
    "#     return round(score, 4)\n",
    "\n",
    "# def compute_metric2_stable(df_actual, df_pred, df_aux):\n",
    "#     merged = (\n",
    "#         df_actual.merge(df_pred, on=[\"country\",\"brand_name\",\"months_postgx\"], how=\"inner\", suffixes=(\"_actual\",\"_predict\"))\n",
    "#                  .merge(df_aux, on=[\"country\",\"brand_name\"], how=\"left\")\n",
    "#     )\n",
    "#     merged[\"start_month\"] = merged.groupby([\"country\",\"brand_name\"])[\"months_postgx\"].transform(\"min\")\n",
    "#     merged = merged[merged[\"start_month\"] == 6].copy()\n",
    "#     pe_series = merged.groupby([\"country\",\"brand_name\",\"bucket\"]).apply(lambda g: _compute_pe_phase1b(g))\n",
    "#     pe_results = pe_series.reset_index(name=\"PE\")\n",
    "#     bucket1 = pe_results[pe_results[\"bucket\"] == 1]\n",
    "#     bucket2 = pe_results[pe_results[\"bucket\"] == 2]\n",
    "#     n1 = bucket1[[\"country\",\"brand_name\"]].drop_duplicates().shape[0]\n",
    "#     n2 = bucket2[[\"country\",\"brand_name\"]].drop_duplicates().shape[0]\n",
    "#     score = 0.0\n",
    "#     if n1 > 0: score += (2.0 / n1) * bucket1[\"PE\"].sum()\n",
    "#     if n2 > 0: score += (1.0 / n2) * bucket2[\"PE\"].sum()\n",
    "#     return round(score, 4)\n",
    "\n",
    "# m1_score_final = compute_metric1_stable(\n",
    "#     df_actual_S1[['country','brand_name','months_postgx','volume_actual']],\n",
    "#     df_pred_S1[['country','brand_name','months_postgx','volume_predict']],\n",
    "#     df_aux_val\n",
    "# )\n",
    "\n",
    "# m2_score_final = compute_metric2_stable(\n",
    "#     df_actual_S2[['country','brand_name','months_postgx','volume_actual']],\n",
    "#     df_pred_S2[['country','brand_name','months_postgx','volume_predict']],\n",
    "#     df_aux_val\n",
    "# )\n",
    "\n",
    "# print(f\"\\nFinal PE Score (Scenario 1, S1): {m1_score_final}\")\n",
    "# print(f\"Final PE Score (Scenario 2, S2): {m2_score_final}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIpTLEKsh2iq"
   },
   "source": [
    "0.2 has been an improvement ( from 0.6 using 400 num_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qyRY9ZFeFKi"
   },
   "outputs": [],
   "source": [
    "# # to check encoding\n",
    "# # 1. Print UNENCODED Features (Pandas DataFrame)\n",
    "# # Shows the original categories and numeric values.\n",
    "# print(\"\\n[A] UNENCODED FEATURES (X1 - First 5 Rows):\")\n",
    "# print(X1.head())\n",
    "# print(\"-\" * 30)\n",
    "# print(f\"Original X1 Shape: {X1.shape}\")\n",
    "\n",
    "# # 2. Print ENCODED Features (NumPy Array)\n",
    "# # This shows the final numerical result fed into the XGBoost model.\n",
    "# print(\"\\n[B] ENCODED FEATURES (X1_enc - First 5 Rows of NumPy Array):\")\n",
    "# print(X1_enc[:5, :10]) # Print first 5 rows and first 10 columns\n",
    "# print(f\"Final X1_enc Shape: {X1_enc.shape}\")\n",
    "# print(\"-\" * 30)\n",
    "\n",
    "# # Optional: Print total number of features created (columns in the final array)\n",
    "# print(f\"Total Features Created After Encoding: {X1_enc.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
